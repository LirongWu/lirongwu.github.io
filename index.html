<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Lirong Wu, 吴立荣, Remote Sensing, Image Processing, Deep Learning, Denoising, Cloud Removal, Hyperspecral, Wuhan University">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./Files/favicon.ico">
<title>Lirong Wu</title>
<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./" style="color:#2a7ce0"><img src="./Files/lirongwu_photo.jpg" alt="" height="285px" /></a>&nbsp;</td>
<td align="left"><p><a href="./" style="color:#2a7ce0"><font size="4">Lirong Wu (</font><font size="4"; font style="font-family:Microsoft YaHei">吴立荣</font><font size="4">)</font></a><br />
<i>Ph.D Candidate, <a href="https://www.zju.edu.cn/" target="_blank" style="color:#2a7ce0">Zhejiang University</a> & <a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Westlake University</a></i>
<br /><br />
<a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Center for Artificial Intelligence Research and Innovation (CAIRI)</a><br />
<a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Westlake University</a><br />
<br />
Location: E2-223, Westlake University, Dunyu Road #600, Hangzhou, Zhejiang, China<br />
<class="staffshortcut">
 <A HREF="#News" style="color:#2a7ce0">News</A> | 
 <A HREF="#Interest" style="color:#2a7ce0">Research Interest</A> | 
 <A HREF="#Education" style="color:#2a7ce0">Education</A> | 
 <A HREF="#Publications" style="color:#2a7ce0">Publications</A> | 
 <A HREF="#Services" style="color:#2a7ce0">Services</A> | 
 <A HREF="#Awards" style="color:#2a7ce0">Awards</A>
<br />
<br />
 
Email: wulirong@westlake.edu.cn (prior); &nbsp;&nbsp;&nbsp;&nbsp; wulirong@zju.edu.cn <br />
[<a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Google Scholar</a>] 
[<a href="https://github.com/LirongWu" target="_blank" style="color:#2a7ce0">GitHub</a>] 
[<a href="https://www.researchgate.net/profile/Lirong-Wu-2" target="_blank" style="color:#2a7ce0">ResearchGate</a>] 
[<a href="https://orcid.org/0000-0001-5551-3194" target="_blank" style="color:#2a7ce0">ORCID</a>] 

<br />
<br />
<i>Welcome to contacting me about research or internship by emails or WeChat (LirongWu_98).</i><br />
</td></tr></table>


<A NAME="News"><h2>News</h2></A>
  <!-- <div style="height:200px;overflow-y:auto;background:#ffffff;"> -->
  <div style="height:300px;overflow-y:auto;">
  <ul>
    <li><b> <font color="#FF0000">[2023.12]</font> </b> One paper on, <i><font color="#2a7ce0">Protein pre-training</font></i>, have been accepted by <b>AAAI 2024</b>. </li>
    <li><b> <font color="#FF0000">[2023.12]</font> </b> Two co-authored papers on, <i><font color="#2a7ce0">AI4Science</font></i>, have been accepted by <b>AAAI 2024</b>. </li>
    <li><b> <font color="#FF0000">[2023.09]</font> </b> Two co-authored papers on, <i><font color="#2a7ce0">AI4Science</font></i>, have been accepted by <b>NeurIPS 2023</b>. </li>
    <li><b> <font color="#FF0000">[2023.09]</font> </b> Two co-authored papers on, <i><font color="#2a7ce0">mixup</font></i> and <i><font color="#2a7ce0">video prediction</font></i>, have been accepted by <b>NeurIPS 2023</b>. </li>
    <li><b> <font color="#FF0000">[2023.08]</font> </b> One paper on, <i><font color="#2a7ce0">protein modeling</font></i>, has been accepted by <b>Communications Biology</b>, congrats to <a href="https://scholar.google.com/citations?user=8vHkc5YAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Fang Wu</a>. </li>
    <li><b> <font color="#FF0000">[2023.06]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">label denoising</font></i>, has been accepted by <b>TKDE</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#FF0000">[2023.06]</font> </b> One paper on, <i><font color="#2a7ce0">graph augmentation</font></i>, has been accepted by <b>ECML 2023</b>. </li>
    <li><b> <font color="#FF0000">[2023.04]</font> </b> One paper on, <i><font color="#2a7ce0">graph knowledge distillation</font></i>, has been accepted by <b>ICML 2023</b>. </li>
    <li><b> <font color="#FF0000">[2023.03]</font> </b> One paper on, <i><font color="#2a7ce0">graph structure learning</font></i>, has been accepted by <b>TNNLS</b>. </li>
    <li><b> <font color="#FF0000">[2022.02]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">graph embedding</font></i>, has been accepted by <b>ICASSP 2023</b>, congrats to <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=6FZh9C8AAAAJ=en" target="_blank" style="color:#2a7ce0">Bozhen Hu</a>. </li>
    <li><b> <font color="#FF0000">[2022.02]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">spatiotemporal prediction</font></i>, has been accepted by <b>CVPR 2023</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#FF0000">[2022.12]</font> </b> One paper on, <i><font color="#2a7ce0">heterogeneous graph</font></i>, has been accepted by <b>TNNLS</b>. </li>
    
    <li><b> <font color="#FF0000">[2022.11]</font> </b> One paper on, <i><font color="#2a7ce0">graph knowledge distillation</font></i>, has been accepted by <b>AAAI 2023 (<font color="#FF0000">Oral</font>)</b>. </li>
    <li><b> <font color="#FF0000">[2022.09]</font> </b> Two papers on, <i><font color="#2a7ce0">graph augmentation and attack</font></i>, have been accepted by <b>NeurIPS 2022 (<font color="#FF0000">Spotlight</font>)</b>. </li>
    <li><b> <font color="#FF0000">[2022.08]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">graph attack</font></i>, has been accepted by <b>CIKM 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=OgIdbfAAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Zihan Liu</a>. </li>
    <li><b> <font color="#FF0000">[2022.08]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">temporal point process</font></i>, has been accepted by <b>TMLR</b>, congrats to <a href="https://scholar.google.com/citations?user=o5A23qIAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Haitao Lin</a>. </li>
    <li><b> <font color="#FF0000">[2022.07]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">mixup augmentation</font></i>, has been accepted by <b>ECCV 2022 (<font color="#FF0000">Oral</font>)</b>, congrats to <a href="https://scholar.google.com/citations?user=EwMGZsgAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Zicheng Liu</a>. </li>
    <li><b> <font color="#FF0000">[2022.06]</font> </b> One paper on, <i><font color="#2a7ce0">class-imbalanced classification</font></i>, has been accepted by <b>ECML 2022 (<font color="#FF0000">Oral</font>)</b>. </li>
    
    <li><b> <font color="#FF0000">[2022.05]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">graph contrastive learning</font></i>, has been accepted by <b>ICML 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#FF0000">[2022.03]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">video prediction</font></i>, has been accepted by <b>CVPR 2022</b>, congrats to <a href="https://scholar.google.com/citations?hl=en&user=4SclT-QAAAAJ" target="_blank" style="color:#2a7ce0">Zhangyang Gao</a>. </li>
    <li><b> <font color="#FF0000">[2022.03]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">semi-supervised learning</font></i>, has been accepted by <b>CVPR 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#FF0000">[2022.02]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">label denoising</font></i>, has been accepted by <b>ICASSP 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#FF0000">[2022.02]</font> </b> One paper on, <i><font color="#2a7ce0">deep clustering</font></i>, has been accepted by <b>TNNLS</b>. </li>
    <li><b> <font color="#FF0000">[2022.01]</font> </b> One paper on, <i><font color="#2a7ce0">graph contrastive learning</font></i>, has been accepted by <b>WWW 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Jun Xia</a>. </li>
    <li><b> <font color="#FF0000">[2022.01]</font> </b> One paper on, <i><font color="#2a7ce0">disentanglement learning</font></i>, has been accepted by <b>NCAA</b>. </li>

    <li><b> <font color="#FF0000">[2021.12]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">spatio-temporal forecasting</font></i>, has been accepted by <b>AAAI 2022</b>, congrats to <a href="https://scholar.google.com/citations?user=o5A23qIAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Haitao Lin</a>. </li>
    <li><b> <font color="#FF0000">[2021.11]</font> </b> One paper on, <i><font color="#2a7ce0">graph self-supervised learning</font></i>, has been accepted by <b>TKDE</b>. </li>
    <li><b> <font color="#FF0000">[2021.10]</font> </b> One paper on, <i><font color="#2a7ce0">deep clustering</font></i>, has been accepted by <b>WACV 2022</b>. </li>
    <li><b> <font color="#FF0000">[2021.08]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">label denoising</font></i>, has been accepted by <b>ACM MM 2021 (<font color="#FF0000">Oral</font>)</b>, congrats to <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Cheng Tan</a>. </li>
    <li><b> <font color="#FF0000">[2021.07]</font> </b> One co-authored paper on, <i><font color="#2a7ce0">invertible learning</font></i>, has been accepted by <b>ECML 2021</b>, congrats to <a href="https://scholar.google.com/citations?user=SKTQTXwAAAAJ&hl=en" target="_blank" style="color:#2a7ce0">Siyuan Li</a>. </li>
    <li><b> <font color="#FF0000">[2020.10]</font> </b> One paper on, <i><font color="#2a7ce0">mass spectrometry</font></i>, has been accepted by <b>JASMS</b>. </li>
    <li><b> <font color="#FF0000">[2020.09]</font> </b> One paper on, <i><font color="#2a7ce0">video compression</font></i>, has been accepted by <b>TCSVT</b>. </li>
    <li><b> <font color="#FF0000">[2020.09]</font> </b> Got my B.E. degree! </li>
    <li><b> <font color="#FF0000">[2019.10]</font> </b> One paper on, <i><font color="#2a7ce0">image compression</font></i>, has been accepted by <b>WACV 2020</b>. </li>
  </ul>
</div>



<script>
	$(function(){
		$(".t").click(function(){
			var children = $(this).siblings(".txt")
			if(children.is(":hidden")){
				children.show();
			}
		})
	})
</script>



 
<A NAME="Interest"><h1>Research Interest</h1></A>
Currently, I focus on the following research topics:
<ul>
<li>Deep Image/Graph Clustering</li>
<li>Graph Self-supervised Learning</li>
<li>Heterogeneous/Homophily Graph Learning</li>
<li>Graph Knowledge Distillation</li>
</ul>
<br />



 
<A NAME="Education"><h1>Education</h1></A>
<ul>
<li>2020.09-present &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ph.D in <a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">CAIRI</a>, <a href="https://www.westlake.edu.cn/" target="_blank" style="color:#2a7ce0">Westlake University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://scholar.google.com/citations?hl=en&user=Y-nyLGIAAAAJ" target="_blank" style="color:#2a7ce0">Stan Z. Li</a></li>
<li>2016.09-2020.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B.E. in <a href="http://www.isee.zju.edu.cn/" target="_blank" style="color:#2a7ce0">ISEE</a>, <a href="https://www.zju.edu.cn/" target="_blank" style="color:#2a7ce0">Zhejiang University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="https://scholar.google.com/citations?hl=en&user=6tSoD98AAAAJ" target="_blank" style="color:#2a7ce0">Kejie Huang</a></li>
</ul>
<br />

 


<A NAME="Publications"><h1>Publications</h1></A>
<p><b><font color="#000000" size=4.5> Journals: </font></b></p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Files/CB_2023_EGNN_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://www.nature.com/articles/s42003-023-05133-1" target="_blank" style="color:#2a7ce0">Integration of pre-trained protein language models into geometric deep learning networks</a></b></font><br>
        <i> Fang Wu, <b>Lirong Wu</b>, Dragomir Radev, Jinbo Xu, and Stan Z. Li </a></i><br><i><b>Communications Biology</b></i><br>
        [<a href= "https://www.nature.com/articles/s42003-023-05133-1" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/smiles724/bottleneck" target="_blank" style="color:#2a7ce0">Code</a>]  
        [<a href="./Files/CB_2023_EGNN_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/TKDE_2023_GNNCleaner_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/abstract/document/10158394" target="_blank" style="color:#2a7ce0">GNN Cleaner: Label Cleaner for Graph Structured Data</a></b></font><br>
        <i> Jun Xia, Haitao Lin, Yongjie Xu, Cheng Tan, <b>Lirong Wu</b>, Siyuan Li, and Stan Z. Li </a></i><br><i><b>IEEE Transactions on Knowledge and Data Engineering (TKDE)</b></i><br>
        [<a href= "https://ieeexplore.ieee.org/abstract/document/10158394" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="./Files/TKDE_2023_GNNCleaner_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/TNNLS_2023_HESGSL_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/abstract/document/10106110" target="_blank" style="color:#2a7ce0">Homophily-Enhanced Self-Supervision for Graph Structure Learning: Insights and Directions</a></b></font><br>
        <i> <b>Lirong Wu</b>, Haitao Lin, Zihan Liu, Zicheng Liu, Yufei Huang, and Stan Z. Li </a></i><br><i><b>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</b></i><br>
        [<a href= "https://ieeexplore.ieee.org/abstract/document/10106110" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/Homophily-Enhanced-Self-supervision" target="_blank" style="color:#2a7ce0">Code</a>]  
        [<a href="./Files/TNNLS_2023_HESGSL_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/TNNLS_2022_RFAGNN_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/abstract/document/10011169" target="_blank" style="color:#2a7ce0">Beyond Homophily: Relation-Based Frequency Adaptive Graph Neural Networks</a></b></font><br>
        <i> <b>Lirong Wu</b>*, Haitao Lin*, Bozhen Hu, Cheng Tan, Zhangyang Gao, and Stan Z. Li </a></i><br><i><b>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</b></i><br>
        [<a href= "https://ieeexplore.ieee.org/abstract/document/10011169" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/RFA-GNN" target="_blank" style="color:#2a7ce0">Code</a>]  
        [<a href="./Files/TNNLS_2022_RFAGNN_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/TMLR_2022_GNTPP_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2208.01874" target="_blank" style="color:#2a7ce0">Exploring Generative Neural Temporal Point Process</a></b></font><br>
        <i> Haitao Lin, <b>Lirong Wu</b>, Guojiang Zhao, Pai Liu, and Stan Z. Li </a></i><br><i><b>Transactions on Machine Learning Research (TMLR)</b></i><br>
        [<a href= "https://arxiv.org/pdf/2208.01874" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/BIRD-TAO/GNTPP" target="_blank" style="color:#2a7ce0">Code</a>]  
        [<a href="./Files/TMLR_2022_GNTPP_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/TNNLS_2022_DCV_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/abstract/document/9732198/" target="_blank" style="color:#2a7ce0">Deep Clustering and Visualization for End-to-End High-Dimensional Data Analysis</a></b></font><br>
        <i> <b>Lirong Wu</b>, Lifan Yun, Guojiang Zhao, Haitao Lin, and Stan Z. Li </a></i><br><i><b>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</b></i><br>
        [<a href= "https://ieeexplore.ieee.org/abstract/document/9732198/" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/DCV" target="_blank" style="color:#2a7ce0">Code</a>]  
        [<a href="./Files/TNNLS_2022_DCV_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/NCAA_2022_MDGNN_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://link.springer.com/article/10.1007/s00521-022-06930-1" target="_blank" style="color:#2a7ce0">Multi-level disentanglement graph neural network</a></b></font><br>
        <i> <b>Lirong Wu</b>, Haitao Lin, Jun Xia, Cheng Tan, and Stan Z. Li </a></i><br><i><b>Neural Computing and Applications (NCAA)</b></i><br>
        [<a href= "https://link.springer.com/article/10.1007/s00521-022-06930-1" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/MD-GNN" target="_blank" style="color:#2a7ce0">Code</a>]  
        [<a href="./Files/NCAA_2022_MDGNN_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>]
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/TKDE_2021_GraphSSL_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2105.07342" target="_blank" style="color:#2a7ce0">Self-supervised Learning on Graphs: Contrastive, Generative,or Predictive</a></b></font><br>
        <i> <b>Lirong Wu</b>, Haitao Lin, Cheng Tan, Zhangyang Gao, and Stan Z. Li </a></i><br><i><b>IEEE Transactions on Knowledge and Data Engineering (TKDE)</b></i><br>
        [<a href= "https://arxiv.org/pdf/2105.07342" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/awesome-graph-self-supervised-learning" target="_blank" style="color:#2a7ce0">Code</a>]  
        [<a href="./Files/TKDE_2021_GraphSSL_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
        <a class="github-button" href="https://github.com/LirongWu/awesome-graph-self-supervised-learning" data-show-count="true" aria-label="Star LirongWu/awesome-graph-self-supervised-learning on GitHub">Star</a>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/JASMS_2021_MS_cover.png" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://pubs.acs.org/doi/pdf/10.1021/jasms.0c00254" target="_blank" style="color:#2a7ce0">Phenotype classification using proteome data in a data-independent acquisition tensor format</a></b></font><br>
        <i> Fangfei Zhang*, Shaoyang Yu*, <b>Lirong Wu</b>*, Zelin Zang, Xiao Yi, et al. </a></i><br><i><b>Journal of the American Society for Mass Spectrometry (JASMS)</b></i><br>
        [<a href= "https://pubs.acs.org/doi/pdf/10.1021/jasms.0c00254" target="_blank" style="color:#2a7ce0">PDF</a>]  
        [<a href="./Files/JASMS_2021_MS_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/TCSVT_2021_DVC_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9208694" target="_blank" style="color:#2a7ce0">Foreground-background Parallel Compression with Residual Encoding for Surveillance Video</a></b></font><br>
        <i> <b>Lirong Wu</b>, Kejie Huang, Haibin Shen, Lianli Gao </a></i><br><i><b>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</b></i><br>
        [<a href= "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9208694" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="./Files/TCSVT_2021_DVC_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>


</ul>
</font>
<br />
<br />
<br />

 
 
<p><b><font color="#000000" size=4.5> Conferences: </font></b></p>
<font size="3"> 
<ul>

<table class="imgtable"><tr><td>
    <img src="./Files/NeurIPS_2023_D3FG_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2306.13769" target="_blank" style="color:#2a7ce0">Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration</a></b></font><br>
        <i> Haitao Lin, Yufei Huang, Haotian Zhang, <b>Lirong Wu</b>, Siyuan Li, et al. </a></i><br><i><b>NeurIPS, 2023</b></i><br>
        [<a href= "https://arxiv.org/pdf/2306.13769" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/BIRD-TAO/D3FG" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/NeurIPS_2023_D3FG_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/NeurIPS_2023_ProteinB_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/forum?id=bqXduvuW5E" target="_blank" style="color:#2a7ce0">ProteinInvBench: Benchmarking Protein Inverse Folding on Diverse Tasks, Models, and Metrics</a></b></font><br>
        <i> Zhangyang Gao, Cheng Tan, Yijie Zhang, Xingran Chen, <b>Lirong Wu</b>, and Stan Z. Li </a></i><br><i><b>NeurIPS, 2023</b></i><br>
        [<a href= "https://openreview.net/forum?id=bqXduvuW5E" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/A4Bio/ProteinInvBench" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/NeurIPS_2023_ProteinB_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/NeurIPS_2023_DM_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/forum?id=YPQg2RTFD8" target="_blank" style="color:#2a7ce0">Harnessing hard mixed samples with decoupled regularizer</a></b></font><br>
        <i> Zicheng Liu, Siyuan Li, Ge Wang, <b>Lirong Wu</b>, Cheng Tan, and Stan Z. Li </a></i><br><i><b>NeurIPS, 2023</b></i><br>
        [<a href= "https://openreview.net/forum?id=YPQg2RTFD8" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/NeurIPS_2023_DM_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/NeurIPS_2023_OpenSTL_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2306.11249" target="_blank" style="color:#2a7ce0">OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning</a></b></font><br>
        <i> Cheng Tan, Siyuan Li, Zhangyang Gao, Wenfei Guan, Zedong Wang, <b>Lirong Wu</b>, et al. </a></i><br><i><b>NeurIPS, 2023</b></i><br>
        [<a href= "https://arxiv.org/pdf/2306.11249" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/NeurIPS_2023_OpenSTL_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ECML_2023_L2A_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://link.springer.com/chapter/10.1007/978-3-031-43418-1_1" target="_blank" style="color:#2a7ce0">Learning to Augment Graph Structure for both Homophily and Heterophily Graphs</a></b></font><br>
        <i> <b>Lirong Wu</b>*, Cheng Tan*, Zicheng Liu, Zhangyang Gao, Haitao Lin, and Stan Z. Li </a></i><br><i><b>ECML, 2022</b></i><br>
        [<a href= "https://link.springer.com/chapter/10.1007/978-3-031-43418-1_1" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/L2A" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ECML_2023_L2A_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ICML_2023_KRD_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2306.05628" target="_blank" style="color:#2a7ce0">Quantifying the Knowledge in GNNs for Reliable Distillation into MLP</a></b></font><br>
        <i> <b>Lirong Wu</b>, Haitao Lin, Yufei Huang, and Stan Z. Li </a></i><br><i><b>ICML, 2023</b></i><br>
        [<a href= "https://arxiv.org/pdf/2306.05628" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/KRD" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ICML_2023_KRD_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ICASSP_2023_DMGAE_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/abstract/document/10095904" target="_blank" style="color:#2a7ce0">Deep Manifold Graph Auto-Encoder For Attributed Graph Embedding</a></b></font><br>
        <i> Bozhen Hu, Zelin Zang, Jun Xia, <b>Lirong Wu</b>, Cheng Tan, and Stan Z. Li </a></i><br><i><b>ICASSP, 2023</b></i><br>
        [<a href= "https://ieeexplore.ieee.org/abstract/document/10095904" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/bozhenhhu/DMVGAE" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ICASSP_2023_DMGAE_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/CVPR_2023_TAU_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2206.12126" target="_blank" style="color:#2a7ce0">Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning</a></b></font><br>
        <i> Cheng Tan*, Zhangyang Gao*, <b>Lirong Wu</b>, Yongjie Xu, Jun Xia, Siyuan Li, and Stan Z. Li </a></i><br><i><b>CVPR, 2023</b></i><br>
        [<a href= "https://arxiv.org/pdf/2206.12126" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/chengtan9907/OpenSTL" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/CVPR_2023_TAU_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/AAAI_2023_FFG2M_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2305.10758" target="_blank" style="color:#2a7ce0">Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and
            Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework</a></b></font><br>
        <i> <b>Lirong Wu</b>, Haitao Lin, Yufei Huang, Tianyu Fan, and Stan Z. Li </a></i><br><i><b>AAAI, 2023 (<font color="#FF0000">Oral</font>)</b></i><br>
        [<a href= "https://arxiv.org/pdf/2305.10758" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/FF-G2M" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/AAAI_2023_FFG2M_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/NeurIPS_2022_KDGA_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/forum?id=7yHte3tH8Xh" target="_blank" style="color:#2a7ce0">Knowledge Distillation Improves Graph Structure Augmentation for Graph Neural Networks</a></b></font><br>
        <i> <b>Lirong Wu</b>, Haitao Lin, Yufei Huang, and Stan Z. Li </a></i><br><i><b>NeurIPS, 2022</b></i><br>
        [<a href= "https://openreview.net/forum?id=7yHte3tH8Xh" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/KDGA" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/NeurIPS_2022_KDGA_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/NeurIPS_2022_GraD_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openreview.net/forum?id=vkGk2HI8oOP" target="_blank" style="color:#2a7ce0">Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias</a></b></font><br>
        <i> Zihan Liu, Yun Luo, <b>Lirong Wu</b>, Zicheng Liu, and Stan Z. Li </a></i><br><i><b>NeurIPS, 2022 (<font color="#FF0000">Spotlight</font>)</b></i><br>
        [<a href= "https://openreview.net/forum?id=vkGk2HI8oOP" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/Zihan-Liu-00/GraD--NeurIPS22" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/NeurIPS_2022_GraD_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/CIKM_2022_AtkSE_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://dl.acm.org/doi/10.1145/3511808.3557238" target="_blank" style="color:#2a7ce0">Are Gradients on Graph Structure Reliable in Gray-box Attacks?</a></b></font><br>
        <i> Zihan Liu, Yun Luo, <b>Lirong Wu</b>, Siyuan Li, Zicheng Liu, and Stan Z. Li </a></i><br><i><b>CIKM, 2022</b></i><br>
        [<a href= "https://dl.acm.org/doi/10.1145/3511808.3557238" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/Zihan-Liu-00/AtkSE--CIKM22" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/CIKM_2022_AtkSE_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ECCV_2022_AutoMix_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2103.13027" target="_blank" style="color:#2a7ce0">AutoMix: Unveiling the Power of Mixup for Stronger Classifiers</a></b></font><br>
        <i> Zicheng Liu*, Siyuan Li*, Di Wu, Zihan Liu, Zhiyuan Chen, <b>Lirong Wu</b>, and Stan Z. Li </a></i><br><i><b>ECCV, 2022 (<font color="#FF0000">Oral</font>)</b></i><br>
        [<a href= "https://arxiv.org/pdf/2103.13027" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/Westlake-AI/openmixup" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ECCV_2022_AutoMix_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] <br>
        <a class="github-button" href="https://github.com/Westlake-AI/openmixup" data-show-count="true" aria-label="Star Westlake-AI/openmixup on GitHub">Star</a>
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ECML_2022_GraphMixup_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2106.11133" target="_blank" style="color:#2a7ce0">GraphMixup: Improving Class-Imbalanced Classification by Self-supervised Context Prediction</a></b></font><br>
        <i> <b>Lirong Wu</b>*, Jun Xia*, Zhangyang Gao, Haitao Lin, Cheng Tan, and Stan Z. Li </a></i><br><i><b>ECML, 2022 (<font color="#FF0000">Oral</font>)</b></i><br>
        [<a href= "https://arxiv.org/pdf/2106.11133" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/GraphMixup" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ECML_2022_GraphMixup_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ICML_2022_ProGCL_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2110.02027" target="_blank" style="color:#2a7ce0">ProGCL: Rethinking Hard Negative Mining in Graph Contrastive Learning</a></b></font><br>
        <i> Jun Xia, <b>Lirong Wu</b>, Ge Wang, Jintao Chen, and Stan Z. Li </a></i><br><i><b>ICML, 2022</b></i><br>
        [<a href= "https://arxiv.org/pdf/2110.02027" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/junxia97/ProGCL" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ICML_2022_ProGCL_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/CVPR_2022_SimVP_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_SimVP_Simpler_Yet_Better_Video_Prediction_CVPR_2022_paper.pdf" target="_blank" style="color:#2a7ce0">SimVP: Simpler Yet Better Video Prediction</a></b></font><br>
        <i> Zhangyang Gao*, Cheng Tan*, <b>Lirong Wu</b>, and Stan Z. Li </a></i><br><i><b>CVPR, 2022</b></i><br>
        [<a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_SimVP_Simpler_Yet_Better_Video_Prediction_CVPR_2022_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/CVPR_2022_SimVP_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/CVPR_2022_HCR_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Tan_Hyperspherical_Consistency_Regularization_CVPR_2022_paper.pdf" target="_blank" style="color:#2a7ce0">Hyperspecral Consistency Regularization</a></b></font><br>
        <i> Cheng Tan*, Zhangyang Gao*, <b>Lirong Wu</b>, Siyuan Li, and Stan Z. Li </a></i><br><i><b>CVPR, 2022</b></i><br>
        [<a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Tan_Hyperspherical_Consistency_Regularization_CVPR_2022_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/chengtan9907/Hyperspherical-Consistency-Regularization" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/CVPR_2022_HCR_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ICASSP_2022_OT_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://ieeexplore.ieee.org/document/9747279" target="_blank" style="color:#2a7ce0">OT Cleaner: Label Correction as Optimal Transport</a></b></font><br>
        <i> Jun Xia*, Cheng Tan*, <b>Lirong Wu</b>, Yongjie Xu, and Stan Z. Li </a></i><br><i><b>ICASSP, 2022</b></i><br>
        [<a href= "https://ieeexplore.ieee.org/document/9747279" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/junxia97/OT-Cleaner" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/ICASSP_2022_OT_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/WWW_2022_SimGRACE_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2202.03104" target="_blank" style="color:#2a7ce0">SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation</a></b></font><br>
        <i> Jun Xia*, <b>Lirong Wu</b>*, Jintao Chen, Bozhen Hu, and Stan Z. Li </a></i><br><i><b>WWW, 2022</b></i><br>
        [<a href= "https://arxiv.org/pdf/2202.03104" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/junxia97/SimGRACE" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/WWW_2022_SimGRACE_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/AAAI_2022_CLCRN_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2101.01000" target="_blank" style="color:#2a7ce0">Conditional Local Convolution for Spatio-temporal Meteorological Forecasting</a></b></font><br>
        <i> Haitao Lin*, Zhangyang Gao*, Yongjie Xu, <b>Lirong Wu</b>, Ling Li, and Stan Z. Li </a></i><br><i><b>AAAI, 2022</b></i><br>
        [<a href= "https://arxiv.org/pdf/2101.01000" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/BIRD-TAO/CLCRN" target="_blank" style="color:#2a7ce0">Code</a>]
        [<a href="./Files/AAAI_2022_CLCRN_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/WACV_2022_GCML_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content/WACV2022/papers/Wu_Generalized_Clustering_and_Multi-Manifold_Learning_With_Geometric_Structure_Preservation_WACV_2022_paper.pdf" target="_blank" style="color:#2a7ce0">Generalized Clustering and Multi-Manifold Learning with Geometric Structure Preservation</a></b></font><br>
        <i> <b>Lirong Wu</b>, Zicheng Liu, Zelin Zang, Jun Xia, Siyuan Li, Stan Z. Li </a></i><br><i><b>WACV, 2022</b></i><br>
        [<a href= "https://openaccess.thecvf.com/content/WACV2022/papers/Wu_Generalized_Clustering_and_Multi-Manifold_Learning_With_Geometric_Structure_Preservation_WACV_2022_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/LirongWu/GCML" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/WACV_2022_GCML_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ACMMM_2021_Co-learning_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://dl.acm.org/doi/pdf/10.1145/3474085.3475622" target="_blank" style="color:#2a7ce0">Co-learning: Learning from Noisy Labels with Self-supervision</a></b></font><br>
        <i> Cheng Tan*, Jun Xia*, <b>Lirong Wu</b>, Stan Z. Li </a></i><br><i><b>ACM MM, 2021 (<font color="#FF0000">Oral</font>)</b></i><br>
        [<a href= "https://dl.acm.org/doi/pdf/10.1145/3474085.3475622" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/chengtan9907/Co-training-based_noisy-label-learning" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/ACMMM_2021_Co-learning_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>]  
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/ECML_2021_INV_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://arxiv.org/pdf/2010.04012" target="_blank" style="color:#2a7ce0">Invertible Manifold Learning for Dimension Reduction</a></b></font><br>
        <i> Siyuan Li, Haitao Lin, Zelin Zang, <b>Lirong Wu</b>, Jun Xia, Stan Z. Li </a></i><br><i><b>ECML, 2021</b></i><br>
        [<a href= "https://arxiv.org/pdf/2010.04012" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="https://github.com/Westlake-AI/inv-ML" target="_blank" style="color:#2a7ce0">Code</a>] 
        [<a href="./Files/ECML_2021_INV_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>

<table class="imgtable"><tr><td>
    <img src="./Files/WACV_2020_GAN_cover.PNG" style="border:1.2px solid #464646;padding:5px;border-radius:14px;box-shadow:1.2px 1.2px #bbbbbb" alt="" width="220px" />&nbsp;</td>
    <td align="left"><p>
        <font size="2pt" face="Georgia"><b><a href= "https://openaccess.thecvf.com/content_WACV_2020/papers/Wu_A_GAN-based_Tunable_Image_Compression_System_WACV_2020_paper.pdf" target="_blank" style="color:#2a7ce0">A Gan-based Tunable Image Compression System</a></b></font><br>
        <i> <b>Lirong Wu</b>, Kejie Huang, Haibin Shen </a></i><br><i><b>WACV, 2020</b></i><br>
        [<a href= "https://openaccess.thecvf.com/content_WACV_2020/papers/Wu_A_GAN-based_Tunable_Image_Compression_System_WACV_2020_paper.pdf" target="_blank" style="color:#2a7ce0">PDF</a>] 
        [<a href="./Files/WACV_2020_GAN_Slide.pdf" target="_blank" style="color:#2a7ce0">Slide</a>] 
        [<a href="./Files/WACV_2020_GAN_Poster.pdf" target="_blank" style="color:#2a7ce0">Poster</a>] 
        [<a href="https://www.youtube.com/watch?v=hrI-WTrZEgM&t=180s" target="_blank" style="color:#2a7ce0">Video</a>] 
        [<a href="./Files/WACV_2020_GAN_bibtex.html" target="_blank" style="color:#2a7ce0">BibTeX</a>] 
</p></td></tr></table>


</ul>
<br />
 


 
<A NAME="Services"><h1>Services</h1></A>
 
<p><b><font color="#000000" size=4.5> Membership: </font></b></p>
<font size="3"> 
<ul>
<li>IEEE, Student Member, 2019-present</li>
</ul>
</font>
<br />
<br />
 
<p><b><font color="#000000" size=4.5> Program committee member | Reviewer </font></b></p>
<font size="3"> 
<ul>
    <li>International Conference on Computer Vision (<b>ICCV</b>), 2023</li>
    <li>ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<b>KDD</b>), 2023</li>
    <li>Conference and Workshop on Neural Information Processing Systems (<b>NeurIPS</b>), 2022, 2023</li>
    <li>International Conference on Machine Learning (<b>ICML</b>), 2022, 2023</li>
    <li>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022, 2023</li>
    <li>IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>)</li>
</ul>
</font>
<br />

  </div>

<!-- <A NAME="Awards"><h1>Awards</h1></A>
<font size="3"> 
<ul>
<li>2017, First Prize of Academic Scholarship, Zhejiang University | <font style="font-family:Microsoft YaHei">学业奖学金一等奖</font></li>
</ul>
</font>
 
<br />
<br />


<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5x3ebj080sx&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
 

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script> -->


<!--
All Rights Reserved by Lirong Wu. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
-->

<!--
<font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2021.09.24</p>
</font>
-->

</body>
</html>
